sparse <- removeSparseTerms(frequencies, 0.995) #quitar palabras que son mencionadas muy poco / deja las palabras mas usadas
sparse #imprime sparse  -- sparce=palabras que aparecen poco
tweetsSparse <- as.data.frame(as.matrix(sparse)) # retornar la variable sparse como una base de datos en formato R
colnames(tweetsSparse) = make.names(colnames(tweetsSparse)) #asinar los nombres de cada palabra al dataframe
tweetsSparse$sentiment <- importdata$sentiment #examina con la base de datos de sentimientos precargada
#modelo de clasificaci?n
#Objetivo: Encontrar  un hiperplano h de dimension (n-1) que separe los ejemplos etiquetados con -1 de los etiquetados con +1
#con un "margen maximo" (P).
#los SVM buscan los puntos mas cercanos entre varias clases. Estos puntos se llaman "los vectores de soporte"
#SVM luego declara que la mejor linea de separaci?n va a ser la linea que divide las dos clases y que al mismo tiempo
#maximiza la distancia del hiperplano a los vectores de soporte (r)
#Ecuaci?n del hiperplano: WX+b=0
#Problema de optimizaci?n: Encuentre W y b  tal que  P=2/[[W]]
#partir la base de datos en entrenamiento y evaluaci?n
#(entrenamiento: ense?arle a la BD como funcionar, evaluaci?n: evaluar como lo hace el modelo y su poder de predicci?n)
set.seed(12)
split <- sample.split(tweetsSparse$sentiment, SplitRatio = 0.8) #decide que observaciones se van para un lado u otro
#plitRatio= 0.8 / el 805 se va a entrenamiento
trainSparse = subset(tweetsSparse, split==TRUE) #particion modelo de entrenamiento
TestSparse = subset(tweetsSparse, split==FALSE) #particion  modelo de evaluaci?n
table(TestSparse$sentiment)
165/328
#algoritmo de clasificacion
# Se debe cargar: library(caret); library(e1071), library(plyr)
SVM <- svm(as.factor(sentiment)~ ., data=trainSparse)  # Entrena al modelo soporte vector machine
summary(SVM) #Describe el modelo
predictSVM <- predict(SVM, newdata = TestSparse)
confusionMatrix(predictSVM,as.factor(TestSparse$sentiment)) #evaluar el desempe?o de los tweets
# ahora se va a crear una nube con las palabras mas repetidas
positive <- subset(tweetsSparse, tweetsSparse$sentiment==1) #crea base de datos de tweets positivos
positive$sentiment <- NULL # Eliminamos la variable sentiment porque no se necesita en la nube de palabras
positivas <- as.data.frame(colSums(positive)) #generar frecuencias de las palabras positivas -- suma de columnas (cada palabra)
positivas$words <- row.names(positivas) #crea variable que se llame words
colnames(positivas) <- c("frecuencia","Palabras")
table(positivas)
# Crear una nube de palabras
# Se debe cargar:
# library(wordcloud)
# library("RColorBrewer")
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 300)
# Crear histograma de frecuencia de palabras
hist(positivas$frecuencia, main ="Frecuencia de palabras positivas", xlab = "Palabras más utilizadas" , ylab = "Frecuencia", col ="light blue",breaks = "Sturges")
#Gráficas de frecuencia
# positivas[1:10, ] %>%
#   ggplot(aes(Palabras, frecuencia)) +
#   geom_bar(stat = "identity", color = "black", fill = "#87CEFA") +
#   geom_text(aes(hjust = 1.3, label = frec)) +
#   coord_flip() +
#   labs(title = "Diez palabras más frecuentes en Niebla",  x = "Palabras", y = "Número de usos")
# Muestra resumen = valor min, máximo; mediana, moda; 1 y 3 quartil
summary(positivas)
# tweets_tidy %>%  ggplot(aes(x = Palabras)) + geom_bar() + coord_flip() + theme_bw()
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 500)
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 100)
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"))
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 150)
View(importdata)
View(positivas)
View(positive)
View(importdata)
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("caTools")
# install.packages("caret")
# install.packages("e1071")
# install.packages("plyr")
# install.packages("magrittr") # Solo se necesita la primera vez que lo usas.
# install.packages("dplyr")    # Instalación alternativa del%>%
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# install.packages("ggplot2")
library(tm) # para mineria de texto
library(SnowballC) # para reducir una palabra a su raíz
library(caTools)
library(caret) # ordenar datos
library(e1071) # machine learning
library(plyr)
library(magrittr) # need to run every time you start R and want to use %>%
library(dplyr)    # alternative, this also loads %>%
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
# posicionamiento del directorio de trabajo
setwd("D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario/datos")
#retorna el directorio sobre el cual se esta trabajando
getwd()
importdata <- read.csv("importado.csv", sep =";") #cargar los datos al objeto tweets ("nombre del archivo.csv", separador)
table(importdata$sentiment) #Cuantos tweets positivos y negativos hay
Corpus= Corpus(VectorSource(importdata$text)) #objeto corpus se le asigna el objeto de tweets // lee los tweets
length(Corpus) #cuenta las palabras tiene el corpus
content(Corpus[[27]]) #imprime la posicion 125
#Preprocesamiento
# Corpus <- tm_map(Corpus, gsub("[[:cntrl:]]", " "))  # Eliminar caracteres especiales de la codificaci?n, como saltos de l?nea y tabulaciones
# Corpus <- tm_map(tolower(Corpus))  # cambia las palabras a minusculas
# Corpus <- tm_map(removeWords(Corpus, words = stopwords("spanish"))) # eliminar palabras vacias, preposiciones y muletillas
# Corpus <- tm_map(removePunctuation(Corpus)) #Elimna la puntuacion
# Corpus <- tm_map(removeNumbers(Corpus)) #Elimna numeros
# Corpus <- tm_map(stripWhitespace(Corpus)) # Elimina espacios vacios excesivos
# Corpus <- tm_map(stemDocument(Corpus)) #acorta las palabras a su raiz "devolvieron" "devolver"
#Corpus <- tm_map(Corpus, PlainTextDocument)  # vuelve el corpus normal para poder visualizarlo
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) # funci?n para eliminar http
Corpus <- tm_map(Corpus, removeURL) # Elimina las palabras que empiezan por "http." seguidas de cualquier cosa que no sea un espacio)
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, content_transformer(tolower)) #procesamiento cambia las palabras a minusculas
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removePunctuation) #quita la puntuacion
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removeNumbers) #quita los numeros
content(Corpus[[27]]) #imprime la posicion 27
stopwords("spanish")[1:50] #preposiciones etc... [1:50] imprime las 50 primeras / stopwords("spanish")
Corpus <- tm_map(Corpus, removeWords, stopwords("spanish")) #remueve las stopwords y otras palabras que se deseen
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stemDocument,language="spanish")  #acorta las palabras a su raiz "devolvieron" "devolver"
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stripWhitespace) #quita los espacios vacios excesivos
content(Corpus[[27]]) #imprime la posicion 27
#clasificaci?n
frequencies <- DocumentTermMatrix(Corpus)  #crear matriz de palabras
frequencies #imprime atributos de frequencies de la matriz; Documents= Filas -twetts, terms= palabras - columnas
inspect(frequencies[15:20, 5:10]) #zoop de la matriz / posiciones [15:20, 5:10]
findFreqTerms(frequencies, lowfreq = 50) # Ver Palabras con mayor frecuencias -- iguales o mayores a 50
sparse <- removeSparseTerms(frequencies, 0.995) #quitar palabras que son mencionadas muy poco / deja las palabras mas usadas
sparse #imprime sparse  -- sparce=palabras que aparecen poco
tweetsSparse <- as.data.frame(as.matrix(sparse)) # retornar la variable sparse como una base de datos en formato R
colnames(tweetsSparse) = make.names(colnames(tweetsSparse)) #asinar los nombres de cada palabra al dataframe
tweetsSparse$sentiment <- importdata$sentiment #examina con la base de datos de sentimientos precargada
#modelo de clasificaci?n
#Objetivo: Encontrar  un hiperplano h de dimension (n-1) que separe los ejemplos etiquetados con -1 de los etiquetados con +1
#con un "margen maximo" (P).
#los SVM buscan los puntos mas cercanos entre varias clases. Estos puntos se llaman "los vectores de soporte"
#SVM luego declara que la mejor linea de separaci?n va a ser la linea que divide las dos clases y que al mismo tiempo
#maximiza la distancia del hiperplano a los vectores de soporte (r)
#Ecuaci?n del hiperplano: WX+b=0
#Problema de optimizaci?n: Encuentre W y b  tal que  P=2/[[W]]
#partir la base de datos en entrenamiento y evaluaci?n
#(entrenamiento: ense?arle a la BD como funcionar, evaluaci?n: evaluar como lo hace el modelo y su poder de predicci?n)
set.seed(12)
split <- sample.split(tweetsSparse$sentiment, SplitRatio = 0.8) #decide que observaciones se van para un lado u otro
#plitRatio= 0.8 / el 805 se va a entrenamiento
trainSparse = subset(tweetsSparse, split==TRUE) #particion modelo de entrenamiento
TestSparse = subset(tweetsSparse, split==FALSE) #particion  modelo de evaluaci?n
table(TestSparse$sentiment)
165/328
#algoritmo de clasificacion
# Se debe cargar: library(caret); library(e1071), library(plyr)
SVM <- svm(as.factor(sentiment)~ ., data=trainSparse)  # Entrena al modelo soporte vector machine
summary(SVM) #Describe el modelo
predictSVM <- predict(SVM, newdata = TestSparse)
confusionMatrix(predictSVM,as.factor(TestSparse$sentiment)) #evaluar el desempe?o de los tweets
# ahora se va a crear una nube con las palabras mas repetidas
positive <- subset(tweetsSparse, tweetsSparse$sentiment==1) #crea base de datos de tweets positivos
positive$sentiment <- NULL # Eliminamos la variable sentiment porque no se necesita en la nube de palabras
positivas <- as.data.frame(colSums(positive)) #generar frecuencias de las palabras positivas -- suma de columnas (cada palabra)
positivas$words <- row.names(positivas) #crea variable que se llame words
colnames(positivas) <- c("frecuencia","Palabras")
table(positivas)
# Crear una nube de palabras
# Se debe cargar:
# library(wordcloud)
# library("RColorBrewer")
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 150)
# Crear histograma de frecuencia de palabras
hist(positivas$frecuencia, main ="Frecuencia de palabras positivas", xlab = "Palabras más utilizadas" , ylab = "Frecuencia", col ="light blue",breaks = "Sturges")
#Gráficas de frecuencia
# positivas[1:10, ] %>%
#   ggplot(aes(Palabras, frecuencia)) +
#   geom_bar(stat = "identity", color = "black", fill = "#87CEFA") +
#   geom_text(aes(hjust = 1.3, label = frec)) +
#   coord_flip() +
#   labs(title = "Diez palabras más frecuentes en Niebla",  x = "Palabras", y = "Número de usos")
# Muestra resumen = valor min, máximo; mediana, moda; 1 y 3 quartil
summary(positivas)
# tweets_tidy %>%  ggplot(aes(x = Palabras)) + geom_bar() + coord_flip() + theme_bw()
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("caTools")
# install.packages("caret")
# install.packages("e1071")
# install.packages("plyr")
# install.packages("magrittr") # Solo se necesita la primera vez que lo usas.
# install.packages("dplyr")    # Instalación alternativa del%>%
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# install.packages("ggplot2")
library(tm) # para mineria de texto
library(SnowballC) # para reducir una palabra a su raíz
library(caTools)
library(caret) # ordenar datos
library(e1071) # machine learning
library(plyr)
library(magrittr) # need to run every time you start R and want to use %>%
library(dplyr)    # alternative, this also loads %>%
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
# posicionamiento del directorio de trabajo
setwd("D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario/datos")
#retorna el directorio sobre el cual se esta trabajando
getwd()
importdata <- read.csv("importado.csv", sep =";") #cargar los datos al objeto tweets ("nombre del archivo.csv", separador)
table(importdata$sentiment) #Cuantos tweets positivos y negativos hay
Corpus= Corpus(VectorSource(importdata$text)) #objeto corpus se le asigna el objeto de tweets // lee los tweets
length(Corpus) #cuenta las palabras tiene el corpus
content(Corpus[[27]]) #imprime la posicion 125
#Preprocesamiento
# Corpus <- tm_map(Corpus, gsub("[[:cntrl:]]", " "))  # Eliminar caracteres especiales de la codificaci?n, como saltos de l?nea y tabulaciones
# Corpus <- tm_map(tolower(Corpus))  # cambia las palabras a minusculas
# Corpus <- tm_map(removeWords(Corpus, words = stopwords("spanish"))) # eliminar palabras vacias, preposiciones y muletillas
# Corpus <- tm_map(removePunctuation(Corpus)) #Elimna la puntuacion
# Corpus <- tm_map(removeNumbers(Corpus)) #Elimna numeros
# Corpus <- tm_map(stripWhitespace(Corpus)) # Elimina espacios vacios excesivos
# Corpus <- tm_map(stemDocument(Corpus)) #acorta las palabras a su raiz "devolvieron" "devolver"
#Corpus <- tm_map(Corpus, PlainTextDocument)  # vuelve el corpus normal para poder visualizarlo
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) # funci?n para eliminar http
Corpus <- tm_map(Corpus, removeURL) # Elimina las palabras que empiezan por "http." seguidas de cualquier cosa que no sea un espacio)
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, content_transformer(tolower)) #procesamiento cambia las palabras a minusculas
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removePunctuation) #quita la puntuacion
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removeNumbers) #quita los numeros
content(Corpus[[27]]) #imprime la posicion 27
stopwords("spanish")[1:50] #preposiciones etc... [1:50] imprime las 50 primeras / stopwords("spanish")
Corpus <- tm_map(Corpus, removeWords, stopwords("spanish")) #remueve las stopwords y otras palabras que se deseen
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stemDocument,language="spanish")  #acorta las palabras a su raiz "devolvieron" "devolver"
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stripWhitespace) #quita los espacios vacios excesivos
content(Corpus[[27]]) #imprime la posicion 27
#clasificaci?n
frequencies <- DocumentTermMatrix(Corpus)  #crear matriz de palabras
frequencies #imprime atributos de frequencies de la matriz; Documents= Filas -twetts, terms= palabras - columnas
inspect(frequencies[15:20, 5:10]) #zoop de la matriz / posiciones [15:20, 5:10]
findFreqTerms(frequencies, lowfreq = 50) # Ver Palabras con mayor frecuencias -- iguales o mayores a 50
sparse <- removeSparseTerms(frequencies, 0.995) #quitar palabras que son mencionadas muy poco / deja las palabras mas usadas
sparse #imprime sparse  -- sparce=palabras que aparecen poco
tweetsSparse <- as.data.frame(as.matrix(sparse)) # retornar la variable sparse como una base de datos en formato R
colnames(tweetsSparse) = make.names(colnames(tweetsSparse)) #asinar los nombres de cada palabra al dataframe
tweetsSparse$sentiment <- importdata$sentiment #examina con la base de datos de sentimientos precargada
#modelo de clasificaci?n
#Objetivo: Encontrar  un hiperplano h de dimension (n-1) que separe los ejemplos etiquetados con -1 de los etiquetados con +1
#con un "margen maximo" (P).
#los SVM buscan los puntos mas cercanos entre varias clases. Estos puntos se llaman "los vectores de soporte"
#SVM luego declara que la mejor linea de separaci?n va a ser la linea que divide las dos clases y que al mismo tiempo
#maximiza la distancia del hiperplano a los vectores de soporte (r)
#Ecuaci?n del hiperplano: WX+b=0
#Problema de optimizaci?n: Encuentre W y b  tal que  P=2/[[W]]
#partir la base de datos en entrenamiento y evaluaci?n
#(entrenamiento: ense?arle a la BD como funcionar, evaluaci?n: evaluar como lo hace el modelo y su poder de predicci?n)
set.seed(12)
split <- sample.split(tweetsSparse$sentiment, SplitRatio = 0.8) #decide que observaciones se van para un lado u otro
#plitRatio= 0.8 / el 805 se va a entrenamiento
trainSparse = subset(tweetsSparse, split==TRUE) #particion modelo de entrenamiento
TestSparse = subset(tweetsSparse, split==FALSE) #particion  modelo de evaluaci?n
table(TestSparse$sentiment)
165/328
#algoritmo de clasificacion
# Se debe cargar: library(caret); library(e1071), library(plyr)
SVM <- svm(as.factor(sentiment)~ ., data=trainSparse)  # Entrena al modelo soporte vector machine
summary(SVM) #Describe el modelo
predictSVM <- predict(SVM, newdata = TestSparse)
confusionMatrix(predictSVM,as.factor(TestSparse$sentiment)) #evaluar el desempe?o de los tweets
# ahora se va a crear una nube con las palabras mas repetidas
positive <- subset(tweetsSparse, tweetsSparse$sentiment==1) #crea base de datos de tweets positivos
positive$sentiment <- NULL # Eliminamos la variable sentiment porque no se necesita en la nube de palabras
positivas <- as.data.frame(colSums(positive)) #generar frecuencias de las palabras positivas -- suma de columnas (cada palabra)
positivas$words <- row.names(positivas) #crea variable que se llame words
colnames(positivas) <- c("frecuencia","Palabras")
table(positivas)
# Crear una nube de palabras
# Se debe cargar:
# library(wordcloud)
# library("RColorBrewer")
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 150)
# Crear histograma de frecuencia de palabras
hist(positivas$frecuencia, main ="Frecuencia de palabras positivas", xlab = "Palabras más utilizadas" , ylab = "Frecuencia", col ="light blue",breaks = "Sturges")
#Gráficas de frecuencia
# positivas[1:10, ] %>%
#   ggplot(aes(Palabras, frecuencia)) +
#   geom_bar(stat = "identity", color = "black", fill = "#87CEFA") +
#   geom_text(aes(hjust = 1.3, label = frec)) +
#   coord_flip() +
#   labs(title = "Diez palabras más frecuentes en Niebla",  x = "Palabras", y = "Número de usos")
# Muestra resumen = valor min, máximo; mediana, moda; 1 y 3 quartil
summary(positivas)
# tweets_tidy %>%  ggplot(aes(x = Palabras)) + geom_bar() + coord_flip() + theme_bw()
View(importdata)
View(positivas)
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("caTools")
# install.packages("caret")
# install.packages("e1071")
# install.packages("plyr")
# install.packages("magrittr") # Solo se necesita la primera vez que lo usas.
# install.packages("dplyr")    # Instalación alternativa del%>%
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# install.packages("ggplot2")
library(tm) # para mineria de texto
library(SnowballC) # para reducir una palabra a su raíz
library(caTools)
library(caret) # ordenar datos
library(e1071) # machine learning
library(plyr)
library(magrittr) # need to run every time you start R and want to use %>%
library(dplyr)    # alternative, this also loads %>%
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
# posicionamiento del directorio de trabajo
setwd("D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario/datos")
#retorna el directorio sobre el cual se esta trabajando
getwd()
importdata <- read.csv("importado.csv", sep =";") #cargar los datos al objeto tweets ("nombre del archivo.csv", separador)
table(importdata$sentiment) #Cuantos tweets positivos y negativos hay
Corpus= Corpus(VectorSource(importdata$text)) #objeto corpus se le asigna el objeto de tweets // lee los tweets
length(Corpus) #cuenta las palabras tiene el corpus
content(Corpus[[27]]) #imprime la posicion 125
#Preprocesamiento
# Corpus <- tm_map(Corpus, gsub("[[:cntrl:]]", " "))  # Eliminar caracteres especiales de la codificaci?n, como saltos de l?nea y tabulaciones
# Corpus <- tm_map(tolower(Corpus))  # cambia las palabras a minusculas
# Corpus <- tm_map(removeWords(Corpus, words = stopwords("spanish"))) # eliminar palabras vacias, preposiciones y muletillas
# Corpus <- tm_map(removePunctuation(Corpus)) #Elimna la puntuacion
# Corpus <- tm_map(removeNumbers(Corpus)) #Elimna numeros
# Corpus <- tm_map(stripWhitespace(Corpus)) # Elimina espacios vacios excesivos
# Corpus <- tm_map(stemDocument(Corpus)) #acorta las palabras a su raiz "devolvieron" "devolver"
#Corpus <- tm_map(Corpus, PlainTextDocument)  # vuelve el corpus normal para poder visualizarlo
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) # funci?n para eliminar http
Corpus <- tm_map(Corpus, removeURL) # Elimina las palabras que empiezan por "http." seguidas de cualquier cosa que no sea un espacio)
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removePunctuation) #quita la puntuacion
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removeNumbers) #quita los numeros
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, content_transformer(tolower)) #procesamiento cambia las palabras a minusculas
content(Corpus[[27]]) #imprime la posicion 27
stopwords("spanish")[1:50] #preposiciones etc... [1:50] imprime las 50 primeras / stopwords("spanish")
Corpus <- tm_map(Corpus, removeWords, c(stopwords("spanish"), "amp","leonesp", "barcelon","leon","hoy","reaiz" )) #remueve las stopwords y otras palabras que se deseen
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stemDocument,language="spanish")  #acorta las palabras a su raiz "devolvieron" "devolver"
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stripWhitespace) #quita los espacios vacios excesivos
content(Corpus[[27]]) #imprime la posicion 27
#clasificaci?n
frequencies <- DocumentTermMatrix(Corpus)  #crear matriz de palabras
frequencies #imprime atributos de frequencies de la matriz; Documents= Filas -twetts, terms= palabras - columnas
inspect(frequencies[15:20, 5:10]) #zoop de la matriz / posiciones [15:20, 5:10]
findFreqTerms(frequencies, lowfreq = 50) # Ver Palabras con mayor frecuencias -- iguales o mayores a 50
sparse <- removeSparseTerms(frequencies, 0.995) #quitar palabras que son mencionadas muy poco / deja las palabras mas usadas
sparse #imprime sparse  -- sparce=palabras que aparecen poco
tweetsSparse <- as.data.frame(as.matrix(sparse)) # retornar la variable sparse como una base de datos en formato R
colnames(tweetsSparse) = make.names(colnames(tweetsSparse)) #asinar los nombres de cada palabra al dataframe
tweetsSparse$sentiment <- importdata$sentiment #examina con la base de datos de sentimientos precargada
#modelo de clasificaci?n
#Objetivo: Encontrar  un hiperplano h de dimension (n-1) que separe los ejemplos etiquetados con -1 de los etiquetados con +1
#con un "margen maximo" (P).
#los SVM buscan los puntos mas cercanos entre varias clases. Estos puntos se llaman "los vectores de soporte"
#SVM luego declara que la mejor linea de separaci?n va a ser la linea que divide las dos clases y que al mismo tiempo
#maximiza la distancia del hiperplano a los vectores de soporte (r)
#Ecuaci?n del hiperplano: WX+b=0
#Problema de optimizaci?n: Encuentre W y b  tal que  P=2/[[W]]
#partir la base de datos en entrenamiento y evaluaci?n
#(entrenamiento: ense?arle a la BD como funcionar, evaluaci?n: evaluar como lo hace el modelo y su poder de predicci?n)
set.seed(12)
split <- sample.split(tweetsSparse$sentiment, SplitRatio = 0.8) #decide que observaciones se van para un lado u otro
#plitRatio= 0.8 / el 805 se va a entrenamiento
trainSparse = subset(tweetsSparse, split==TRUE) #particion modelo de entrenamiento
TestSparse = subset(tweetsSparse, split==FALSE) #particion  modelo de evaluaci?n
table(TestSparse$sentiment)
165/328
#algoritmo de clasificacion
# Se debe cargar: library(caret); library(e1071), library(plyr)
SVM <- svm(as.factor(sentiment)~ ., data=trainSparse)  # Entrena al modelo soporte vector machine
summary(SVM) #Describe el modelo
predictSVM <- predict(SVM, newdata = TestSparse)
confusionMatrix(predictSVM,as.factor(TestSparse$sentiment)) #evaluar el desempe?o de los tweets
# ahora se va a crear una nube con las palabras mas repetidas
positive <- subset(tweetsSparse, tweetsSparse$sentiment==1) #crea base de datos de tweets positivos
positive$sentiment <- NULL # Eliminamos la variable sentiment porque no se necesita en la nube de palabras
positivas <- as.data.frame(colSums(positive)) #generar frecuencias de las palabras positivas -- suma de columnas (cada palabra)
positivas$words <- row.names(positivas) #crea variable que se llame words
colnames(positivas) <- c("frecuencia","Palabras")
table(positivas)
# Crear una nube de palabras
# Se debe cargar:
# library(wordcloud)
# library("RColorBrewer")
wordcloud(positivas$Palabras, positivas$frecuencia, random.order = FALSE, colors = brewer.pal(8,"Dark2"), max.words = 150)
# Crear histograma de frecuencia de palabras
hist(positivas$frecuencia, main ="Frecuencia de palabras positivas", xlab = "Palabras más utilizadas" , ylab = "Frecuencia", col ="light blue",breaks = "Sturges")
#Gráficas de frecuencia
# positivas[1:10, ] %>%
#   ggplot(aes(Palabras, frecuencia)) +
#   geom_bar(stat = "identity", color = "black", fill = "#87CEFA") +
#   geom_text(aes(hjust = 1.3, label = frec)) +
#   coord_flip() +
#   labs(title = "Diez palabras más frecuentes en Niebla",  x = "Palabras", y = "Número de usos")
# Muestra resumen = valor min, máximo; mediana, moda; 1 y 3 quartil
summary(positivas)
# tweets_tidy %>%  ggplot(aes(x = Palabras)) + geom_bar() + coord_flip() + theme_bw()
View(tweetsSparse)
View(tweetsSparse)
View(TestSparse)
View(trainSparse)
View(positive)
View(positivas)
shiny::runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario')
runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario')
library(tm) # para mineria de texto
library(SnowballC) # para reducir una palabra a su raíz
library(caTools) # Herramientas estadísticas de ventanas móviles, GIF, Base64, ROC AUC, etc.
library(caret) # ordenar datos
library(e1071) # Paquete para machine learning (Aprendizaje de máquina)
library(plyr) # Herramientas para dividir, aplicar y combinar datos
library(magrittr) # need to run every time you start R and want to use %>%
library(dplyr)    # alternative, this also loads %>%
library(wordcloud)# Crear nubes de palabras, visualizar diferencias y similitudes entre documentos.
library(RColorBrewer)# Crea paletas de colores para aplicar a mapas temáticos y textos.
library(ggplot2)# Paquete de visualización de datos. Crea gráficas
setwd("D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario/datos")
getwd()
importdata <- read.csv("importado.csv", sep =";") #cargar los datos al objeto tweets ("nombre del archivo.csv", separador)
table(importdata$sentiment) #Cuantos tweets positivos y negativos hay
Corpus= Corpus(VectorSource(importdata$text)) #objeto corpus se le asigna el objeto de tweets // lee los tweets
View(Corpus)
length(Corpus) #cuenta las palabras tiene el corpus
content(Corpus[[27]]) #imprime la posicion 125
#Corpus <- tm_map(Corpus, PlainTextDocument)  # vuelve el corpus normal para poder visualizarlo
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) # funci?n para eliminar http
Corpus <- tm_map(Corpus, removeURL) # Elimina las palabras que empiezan por "http." seguidas de cualquier cosa que no sea un espacio)
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removePunctuation) #quita la puntuacion
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, removeNumbers) #quita los numeros
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, content_transformer(tolower)) #procesamiento cambia las palabras a minusculas
content(Corpus[[27]]) #imprime la posicion 27
stopwords("spanish")[1:50] #preposiciones etc... [1:50] imprime las 50 primeras / stopwords("spanish")
Corpus <- tm_map(Corpus, removeWords, c(stopwords("spanish"), "amp","leonesp", "barcelon","leon","hoy","reaiz" )) #remueve las stopwords y otras palabras que se deseen
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stemDocument,language="spanish")  #acorta las palabras a su raiz "devolvieron" "devolver"
content(Corpus[[27]]) #imprime la posicion 27
Corpus <- tm_map(Corpus, stripWhitespace) #quita los espacios vacios excesivos
content(Corpus[[27]]) #imprime la posicion 27
frequencies <- DocumentTermMatrix(Corpus)  #crear matriz de palabras
View(frequencies)
frequencies #imprime atributos de frequencies de la matriz; Documents= Filas -twetts, terms= palabras - columnas
inspect(frequencies[15:20, 5:10]) #zoop de la matriz / posiciones [15:20, 5:10]
findFreqTerms(frequencies, lowfreq = 50) # Ver Palabras con mayor frecuencias -- iguales o mayores a 50
sparse <- removeSparseTerms(frequencies, 0.995) #quitar palabras que son mencionadas muy poco / deja las palabras mas usadas
sparse #imprime sparse  -- sparce=palabras que aparecen poco
tweetsSparse <- as.data.frame(as.matrix(sparse)) # retornar la variable sparse como una base de datos en formato R
View(tweetsSparse)
colnames(tweetsSparse) = make.names(colnames(tweetsSparse)) #asinar los nombres de cada palabra al dataframe
View(tweetsSparse)
tweetsSparse$sentiment <- importdata$sentiment #examina con la base de datos de sentimientos precargada
View(tweetsSparse)
set.seed(12)
split <- sample.split(tweetsSparse$sentiment, SplitRatio = 0.8) #decide que observaciones se van para un lado u otro
#splitRatio= 0.8 / el 80% se va a entrenamiento
trainSparse = subset(tweetsSparse, split==TRUE) #particion modelo de entrenamiento
TestSparse = subset(tweetsSparse, split==FALSE) #particion  modelo de evaluaci?n
table(TestSparse$sentiment)
165/328
SVM <- svm(as.factor(sentiment)~ ., data=trainSparse)  # Entrena al modelo soporte vector machine
summary(SVM) #Describe el modelo
predictSVM <- predict(SVM, newdata = TestSparse)
confusionMatrix(predictSVM,as.factor(TestSparse$sentiment)) #evaluar el desempe?o de los tweets
table(TestSparse$sentiment)
runApp('D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario')
runApp('D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario')
runApp('D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario')
runApp('D:/Estudios/UNAD/Trabajo de grado/Proyecto final/Proyecto R-Twitter/analisis_sentimientos/Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp()
runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp()
runApp('Interfaz_usuario')
shiny::runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
runApp('Interfaz_usuario')
